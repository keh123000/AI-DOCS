## LoRA微调文生文模型Bad Cases解决方案

### 核心设计思路

遵循 **Data-Centric AI（以数据为中心）** 的原则，按照以下优先级进行优化：
**数据诊断与清洗 (P0) > 数据增强与配比 (P1) > 训练策略调整 (P2) > 工程兜底 (P3)**



---

### 第一阶段：深度诊断与归因 (Diagnosis)

在动手修改模型前，必须先明确“Bad Cases”属于哪一类，不同类型的成因对应完全不同的解法。

1.  **分布外样本 (OOD - Out of Distribution)**
    * **特征**：样本的句式、长度、行业术语或领域知识在训练集中从未出现过。
    * **成因**：模型“没见过”，泛化能力不足。
    * **对策**：必须补充此类数据（见第二阶段）。

2.  **数据冲突/歧义 (Data Ambiguity)**
    * **特征**：针对类似的输入，模型输出了错误答案，或者出现了幻觉。
    * **成因**：训练集中存在与Bad Case相似的输入，但标注的输出截然不同（Label Noise），导致模型“困惑”。
    * **对策**：清洗冲突数据，保持一致性。

3.  **难例 (Hard Examples)**
    * **特征**：需要多步推理或复杂逻辑，模型只学会了浅层语义，输出逻辑断裂。
    * **成因**：LoRA 参数量（Rank）不足，或模型本身能力瓶颈。
    * **对策**：调整 LoRA 参数或使用 CoT（思维链）数据。

---

### 第二阶段：数据侧优化方案 (Data Optimization)

**80% 的问题可以通过优化数据解决。**

#### 1. “定向增强”策略 (Targeted Augmentation)
* **合成数据**：利用 GPT-4 或 Qwen-Max 等强模型，针对提取出的 Bad Cases，进行改写和扩充（Data Augmentation）。将原本只有几十条的错误样本扩充至几百条。
* **上采样 (Upsampling)**：如果由于特定样本数量太少（如占比<1%）导致被模型忽略，可简单粗暴地复制这部分数据，将其在训练集中的权重提升至 5%-10%，强迫模型关注。

#### 2. “数据一致性”清洗
* **去噪**：检查训练集中是否存在与 Bad Cases **输入相似但输出质量低**的数据，予以剔除。
* **格式统一**：确保所有指令（Instruction）格式统一，建立清晰的 `Instruction -> Input -> Output` 层级。

#### 3. “防遗忘”混合策略 (Data Mixing)
* **Replay Strategy**：为了防止在修复特定问题时导致通用能力下降（灾难性遗忘），必须在微调数据中**混合 10%-30% 的通用高质量数据**（或之前的优质训练数据）。
* **黄金比例**：建议尝试 `70% 针对性增强数据 + 30% 通用/原始数据` 的配比。

---

### 第三阶段：模型与训练策略调整 (Model Strategy)

如果数据质量已达标但效果仍不佳，则需调整 Qwen3 LoRA 的训练配置。

#### 1. LoRA 配置调优
* **提升 Rank ($r$)**：如果 Bad Cases 涉及复杂逻辑推理，当前的 Rank (如 $r=8$) 可能限制了模型的拟合能力。建议尝试将 Rank 提升至 **32, 64 甚至 128**。
* **调整 Alpha**：适当调高 LoRA Alpha 值（通常设为 Rank 的 2 倍，如 $\alpha=2r$），增强新权重对模型的影响。
* **Target Modules**：不要仅微调 `q_proj, v_proj`，建议覆盖所有线性层（`all-linear`），包括 `k, o, gate, up, down`，这对 Qwen 系列模型提升显著。

#### 2. 困难样本挖掘 (Hard Example Mining)
* **加权 Loss**：在计算损失函数时，增加 Bad Cases 样本的权重。
* **构建课程学习 (Curriculum Learning)**：先用通用数据训练，再用难例数据进行第二阶段微调（SFT Phase 2）。

#### 3. 学习率策略
* **调低学习率**：如果模型在 Bad Cases 上表现为“胡言乱语”而非“没学会”，可能是学习率过高破坏了权重。尝试降低 LR（如从 $5e^{-5}$ 降至 $1e^{-5}$）。
* **分层学习率**：底层参数使用极小学习率，分类头使用稍大学习率。

---

### 第四阶段：工程与推理兜底 (Engineering & Inference)

当微调达到瓶颈，利用工程手段弥补模型缺陷。

#### 1. RAG (检索增强生成)
* 如果 Bad Cases 是因为**缺乏特定知识**（如公司内部最新条文），微调很难注入知识。
* **方案**：建立向量库，将相关知识检索后作为 Context 拼接到 Prompt 中。

#### 2. Few-Shot Prompting (推理时干预)
* 在推理阶段的 Prompt 中，动态插入 1-2 个与当前输入相似的**正确示例**（In-Context Learning）。微调后的模型通常对 Prompt 中的示例极度敏感，能瞬间修正格式或逻辑。

#### 3. 路由机制 (Router)
* 训练一个极小的分类器（Classifier）。当检测到输入属于“易错样本”类别时，路由到更强的模型（如 Qwen-Max 或 GPT-4）处理，或通过规则处理。

---

### 总结：实施路线图

| 步骤 | 行动项 | 关键点 | 预期成本 |
| :--- | :--- | :--- | :--- |
| **Step 1** | **聚类分析** | 将 Bad Cases 分类，确定是数据缺失还是逻辑太难 | 低 |
| **Step 2** | **数据修复** | 合成/扩充难例数据，清洗冲突数据 | 中 |
| **Step 3** | **混合微调** | 加入 20% 通用数据防止遗忘，上采样难例数据 | 中 (需重训) |
| **Step 4** | **参数调整** | 调大 LoRA Rank (至64+)，调低学习率 | 高 (需重训) |
| **Step 5** | **工程兜底** | 部署 RAG 或在 Prompt 中加入 One-shot 示例 | 低 (推理端) |

**建议起手式**：先提取 20-50 条 Bad Cases，利用 GPT-4 扩充至 200 条左右，按 1:5 的比例混合原有数据，使用稍大的 Rank ($r=32$) 进行一次快速验证。
